{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Milestone 2: Evaluation Script and Baselines\n",
        "\n",
        "This notebook implements:\n",
        "1. Evaluation script with Recall@10, Exact Match, Span F1, and nDCG metrics\n",
        "2. Simple baseline (TF-IDF retrieval)\n",
        "3. Strong baseline (BM25 retrieval)"
      ],
      "metadata": {
        "id": "EVUkj24TE8rD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Installation"
      ],
      "metadata": {
        "id": "4OLXvo0lFBsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing required packages\n",
        "!pip install rank-bm25 nltk scikit-learn numpy pandas -q"
      ],
      "metadata": {
        "id": "iWI8UuEJE_l0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Tuple, Set\n",
        "from collections import defaultdict\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Downloading stopwords\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except:\n",
        "    import ssl\n",
        "    try:\n",
        "        _create_unverified_https_context = ssl._create_unverified_context\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    else:\n",
        "        ssl._create_default_https_context = _create_unverified_https_context\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "3QUTGbraFJHa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading Utilities"
      ],
      "metadata": {
        "id": "nHy9m_ZqFHU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_test_benchmark(benchmark_path: str) -> List[Dict]:\n",
        "    \"\"\"Load test benchmark JSON file.\"\"\"\n",
        "    with open(benchmark_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data.get('tests', data)\n",
        "\n",
        "def load_corpus_file(corpus_path: str) -> str:\n",
        "    \"\"\"Load a corpus text file.\"\"\"\n",
        "    with open(corpus_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        return f.read()\n",
        "\n",
        "def get_passage_from_span(text: str, span: List[int]) -> str:\n",
        "    \"\"\"Extract passage from text using character span.\"\"\"\n",
        "    start, end = span\n",
        "    return text[start:end]\n",
        "\n",
        "def load_training_data(csv_path: str, n_samples: int = None) -> pd.DataFrame:\n",
        "    \"\"\"Load training data from CSV.\"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if n_samples:\n",
        "        df = df.head(n_samples)\n",
        "    return df"
      ],
      "metadata": {
        "id": "oCUz5YorFU4k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing"
      ],
      "metadata": {
        "id": "7cOiWMfDFYJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text: str, lower: bool = True, remove_stopwords: bool = False) -> str:\n",
        "    \"\"\"Preprocess text for retrieval.\"\"\"\n",
        "    if lower:\n",
        "        text = text.lower()\n",
        "    # Removing extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def tokenize(text: str, remove_stopwords: bool = False) -> List[str]:\n",
        "    \"\"\"Tokenize text.\"\"\"\n",
        "    text = preprocess_text(text, lower=True)\n",
        "    tokens = word_tokenize(text)\n",
        "    if remove_stopwords:\n",
        "        tokens = [t for t in tokens if t not in stop_words and t.isalnum()]\n",
        "    else:\n",
        "        tokens = [t for t in tokens if t.isalnum()]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "LYbWPLqUFZ4i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Metrics"
      ],
      "metadata": {
        "id": "-oppbKZzFd7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exact_match(predicted: str, gold: str) -> bool:\n",
        "    \"\"\"Check if predicted text exactly matches gold text.\"\"\"\n",
        "    return predicted.strip().lower() == gold.strip().lower()\n",
        "\n",
        "def span_f1(predicted: str, gold: str) -> float:\n",
        "    \"\"\"\n",
        "    Compute F1 score based on token overlap between predicted and gold spans.\n",
        "\n",
        "    F1 = 2 * (precision * recall) / (precision + recall)\n",
        "    precision = |predicted_tokens ∩ gold_tokens| / |predicted_tokens|\n",
        "    recall = |predicted_tokens ∩ gold_tokens| / |gold_tokens|\n",
        "    \"\"\"\n",
        "    pred_tokens = set(tokenize(predicted, remove_stopwords=False))\n",
        "    gold_tokens = set(tokenize(gold, remove_stopwords=False))\n",
        "\n",
        "    if len(pred_tokens) == 0 and len(gold_tokens) == 0:\n",
        "        return 1.0\n",
        "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    intersection = pred_tokens & gold_tokens\n",
        "    precision = len(intersection) / len(pred_tokens) if len(pred_tokens) > 0 else 0.0\n",
        "    recall = len(intersection) / len(gold_tokens) if len(gold_tokens) > 0 else 0.0\n",
        "\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def recall_at_k(retrieved_passages: List[str], gold_passages: List[str], k: int = 10) -> float:\n",
        "    \"\"\"\n",
        "    Compute Recall@K: fraction of gold passages found in top K retrieved passages.\n",
        "\n",
        "    Recall@K = |{gold_passages} ∩ {top_k_retrieved}| / |{gold_passages}|\n",
        "    \"\"\"\n",
        "    if len(gold_passages) == 0:\n",
        "        return 1.0 if len(retrieved_passages) == 0 else 0.0\n",
        "\n",
        "    top_k = retrieved_passages[:k]\n",
        "\n",
        "    # Normalizing and checking for matches\n",
        "    gold_normalized = [preprocess_text(g).strip() for g in gold_passages]\n",
        "    retrieved_normalized = [preprocess_text(r).strip() for r in top_k]\n",
        "\n",
        "    # Counting how many gold passages appear in top K\n",
        "    matches = 0\n",
        "    for gold in gold_normalized:\n",
        "        # Checking for exact or near-exact match\n",
        "        for ret in retrieved_normalized:\n",
        "            if gold in ret or ret in gold or gold == ret:\n",
        "                matches += 1\n",
        "                break\n",
        "\n",
        "    return matches / len(gold_passages)\n",
        "\n",
        "def ndcg_at_k(retrieved_passages: List[str], gold_passages: List[str], k: int = 10) -> float:\n",
        "    \"\"\"\n",
        "    Compute Normalized Discounted Cumulative Gain (nDCG) at K.\n",
        "\n",
        "    DCG@K = sum(rel_i / log2(i+1)) for i in [1, K]\n",
        "    nDCG@K = DCG@K / IDCG@K\n",
        "\n",
        "    where rel_i = 1 if passage i is relevant (in gold), 0 otherwise\n",
        "    \"\"\"\n",
        "    if len(gold_passages) == 0:\n",
        "        return 1.0 if len(retrieved_passages) == 0 else 0.0\n",
        "\n",
        "    top_k = retrieved_passages[:k]\n",
        "\n",
        "    # Normalizing passages\n",
        "    gold_normalized = [preprocess_text(g).strip() for g in gold_passages]\n",
        "    retrieved_normalized = [preprocess_text(r).strip() for r in top_k]\n",
        "\n",
        "    # Computing relevance scores\n",
        "    relevances = []\n",
        "    for ret in retrieved_normalized:\n",
        "        is_relevant = False\n",
        "        for gold in gold_normalized:\n",
        "            if gold in ret or ret in gold or gold == ret:\n",
        "                is_relevant = True\n",
        "                break\n",
        "        relevances.append(1.0 if is_relevant else 0.0)\n",
        "\n",
        "    # Computing DCG@K\n",
        "    dcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(relevances))\n",
        "\n",
        "    # Computing IDCG@K (ideal: all relevant items first)\n",
        "    num_relevant = int(min(sum(relevances), k))\n",
        "    idcg = sum(1.0 / np.log2(i + 2) for i in range(num_relevant))\n",
        "\n",
        "    if idcg == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return dcg / idcg"
      ],
      "metadata": {
        "id": "9J5g-eAGFfvS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Script (score.py)"
      ],
      "metadata": {
        "id": "f0_QYnPXFrWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_retrieval(\n",
        "    predictions: List[Dict],\n",
        "    gold_standard: List[Dict],\n",
        "    k: int = 10\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluate retrieval system performance.\n",
        "\n",
        "    Args:\n",
        "        predictions: List of dicts with 'query', 'retrieved_passages' (list of top passages)\n",
        "        gold_standard: List of dicts with 'query', 'snippets' (list with 'answer' field)\n",
        "        k: Number of top results to consider for Recall@K and nDCG@K\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with evaluation metrics\n",
        "    \"\"\"\n",
        "    if len(predictions) != len(gold_standard):\n",
        "        raise ValueError(f\"Mismatch: {len(predictions)} predictions vs {len(gold_standard)} gold examples\")\n",
        "\n",
        "    exact_matches = []\n",
        "    span_f1_scores = []\n",
        "    recall_at_k_scores = []\n",
        "    ndcg_at_k_scores = []\n",
        "\n",
        "    for pred, gold in zip(predictions, gold_standard):\n",
        "        # Getting gold answers\n",
        "        gold_answers = [snippet['answer'] for snippet in gold.get('snippets', [])]\n",
        "\n",
        "        if len(gold_answers) == 0:\n",
        "            continue\n",
        "\n",
        "        # Getting retrieved passages\n",
        "        retrieved = pred.get('retrieved_passages', [])\n",
        "\n",
        "        if len(retrieved) == 0:\n",
        "            exact_matches.append(0.0)\n",
        "            span_f1_scores.append(0.0)\n",
        "            recall_at_k_scores.append(0.0)\n",
        "            ndcg_at_k_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        # Exact Match: checking if top-1 matches any gold answer\n",
        "        top_pred = retrieved[0] if retrieved else \"\"\n",
        "        em = any(exact_match(top_pred, gold_ans) for gold_ans in gold_answers)\n",
        "        exact_matches.append(1.0 if em else 0.0)\n",
        "\n",
        "        # Span F1: average F1 with best matching gold answer\n",
        "        best_f1 = max([span_f1(top_pred, gold_ans) for gold_ans in gold_answers])\n",
        "        span_f1_scores.append(best_f1)\n",
        "\n",
        "        # Recall@K\n",
        "        rec_k = recall_at_k(retrieved, gold_answers, k=k)\n",
        "        recall_at_k_scores.append(rec_k)\n",
        "\n",
        "        # nDCG@K\n",
        "        ndcg_k = ndcg_at_k(retrieved, gold_answers, k=k)\n",
        "        ndcg_at_k_scores.append(ndcg_k)\n",
        "\n",
        "    return {\n",
        "        'exact_match': np.mean(exact_matches),\n",
        "        'span_f1': np.mean(span_f1_scores),\n",
        "        f'recall@{k}': np.mean(recall_at_k_scores),\n",
        "        f'ndcg@{k}': np.mean(ndcg_at_k_scores),\n",
        "        'num_examples': len(predictions)\n",
        "    }\n",
        "\n",
        "def load_predictions(predictions_path: str) -> List[Dict]:\n",
        "    \"\"\"Load predictions from JSON file.\"\"\"\n",
        "    with open(predictions_path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def load_gold_standard(gold_path: str) -> List[Dict]:\n",
        "    \"\"\"Load gold standard from benchmark JSON file.\"\"\"\n",
        "    return load_test_benchmark(gold_path)"
      ],
      "metadata": {
        "id": "IlReFzmLFtjL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TFIDFRetriever:\n",
        "    \"\"\"\n",
        "    Simple TF-IDF based retrieval baseline.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_features: int = 5000):\n",
        "        self.vectorizer = TfidfVectorizer(\n",
        "            max_features=max_features,\n",
        "            stop_words='english',\n",
        "            ngram_range=(1, 2),\n",
        "            lowercase=True\n",
        "        )\n",
        "        self.corpus_texts = []\n",
        "        self.corpus_vectors = None\n",
        "\n",
        "    def index(self, passages: List[str]):\n",
        "        \"\"\"Index a collection of passages.\"\"\"\n",
        "        self.corpus_texts = passages\n",
        "        self.corpus_vectors = self.vectorizer.fit_transform(passages)\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 10) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Retrieve top-k passages for a query.\"\"\"\n",
        "        if self.corpus_vectors is None:\n",
        "            raise ValueError(\"Index must be built before retrieval\")\n",
        "\n",
        "        query_vector = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_vector, self.corpus_vectors).flatten()\n",
        "\n",
        "        top_indices = np.argsort(similarities)[::-1][:k]\n",
        "\n",
        "        results = [(self.corpus_texts[i], similarities[i]) for i in top_indices]\n",
        "        return results"
      ],
      "metadata": {
        "id": "4mn98s5fF8Bn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strong Baseline: BM25 Retrieval"
      ],
      "metadata": {
        "id": "o4z8L1JGF2Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BM25Retriever:\n",
        "    \"\"\"\n",
        "    BM25 (Best Matching 25) retrieval baseline.\n",
        "    BM25 is a ranking function used to estimate the relevance of documents.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, k1: float = 1.5, b: float = 0.75):\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "        self.bm25 = None\n",
        "        self.corpus_texts = []\n",
        "\n",
        "    def index(self, passages: List[str]):\n",
        "        \"\"\"Index a collection of passages.\"\"\"\n",
        "        self.corpus_texts = passages\n",
        "        # Tokenizing passages\n",
        "        tokenized_passages = [tokenize(p, remove_stopwords=True) for p in passages]\n",
        "        self.bm25 = BM25Okapi(tokenized_passages, k1=self.k1, b=self.b)\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 10) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Retrieve top-k passages for a query.\"\"\"\n",
        "        if self.bm25 is None:\n",
        "            raise ValueError(\"Index must be built before retrieval\")\n",
        "\n",
        "        query_tokens = tokenize(query, remove_stopwords=True)\n",
        "        scores = self.bm25.get_scores(query_tokens)\n",
        "\n",
        "        top_indices = np.argsort(scores)[::-1][:k]\n",
        "\n",
        "        results = [(self.corpus_texts[i], scores[i]) for i in top_indices]\n",
        "        return results"
      ],
      "metadata": {
        "id": "zZtFVRWoGAgL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation for Evaluation"
      ],
      "metadata": {
        "id": "yYeP9U4HGDpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text: str, chunk_size: int = 500) -> List[str]:\n",
        "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(words), chunk_size // 2):\n",
        "        # 50% overlap\n",
        "        chunk = ' '.join(words[i:i + chunk_size])\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk)\n",
        "        if i + chunk_size >= len(words):\n",
        "            break\n",
        "\n",
        "    return chunks if chunks else [text]\n",
        "\n",
        "def prepare_corpus_from_benchmark(\n",
        "    benchmark_path: str,\n",
        "    corpus_dir: str,\n",
        "    chunk_size: int = 500\n",
        ") -> Tuple[List[str], Dict[str, List[str]]]:\n",
        "    \"\"\"\n",
        "    Prepare corpus by chunking documents and mapping queries to gold passages.\n",
        "\n",
        "    Returns:\n",
        "        corpus_passages: List of all passages in corpus\n",
        "        query_to_gold: Mapping from query to list of gold answer passages\n",
        "    \"\"\"\n",
        "    tests = load_test_benchmark(benchmark_path)\n",
        "\n",
        "    corpus_passages = []\n",
        "    passage_to_idx = {}\n",
        "    query_to_gold = {}\n",
        "    processed_files = set()\n",
        "\n",
        "    # Processing each test case\n",
        "    for test in tests:\n",
        "        query = test['query']\n",
        "        gold_answers = []\n",
        "\n",
        "        for snippet in test.get('snippets', []):\n",
        "            file_path = snippet['file_path']\n",
        "            span = snippet['span']\n",
        "            answer = snippet['answer']\n",
        "\n",
        "            # Constructing full path\n",
        "            full_path = os.path.join(corpus_dir, file_path)\n",
        "\n",
        "            if os.path.exists(full_path):\n",
        "                # Loading document and extracting passage\n",
        "                doc_text = load_corpus_file(full_path)\n",
        "                gold_passage = get_passage_from_span(doc_text, span)\n",
        "                gold_answers.append(gold_passage)\n",
        "\n",
        "                # Chunk the document and adding to corpus (only once per file)\n",
        "                if full_path not in processed_files:\n",
        "                    # Splitting document into chunks\n",
        "                    chunks = chunk_text(doc_text, chunk_size)\n",
        "                    corpus_passages.extend(chunks)\n",
        "                    processed_files.add(full_path)\n",
        "\n",
        "        if gold_answers:\n",
        "            query_to_gold[query] = gold_answers\n",
        "\n",
        "    return corpus_passages, query_to_gold"
      ],
      "metadata": {
        "id": "kd20Zz_EGF41"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Baselines on Test Data"
      ],
      "metadata": {
        "id": "HP0KCC3lGVvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# Uploading the ZIP file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extracting the uploaded ZIP file\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        with zipfile.ZipFile(io.BytesIO(uploaded[filename]), 'r') as zip_ref:\n",
        "            zip_ref.extractall('unzipped')\n",
        "        print(f\"Extracted {filename} to /content/unzipped\")\n",
        "    else:\n",
        "        print(f\"{filename} is not a ZIP file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "TR0CAmusseZw",
        "outputId": "ff53e63c-a34d-469c-9322-ac1c3cadab56"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-27d21df8-653f-4533-820b-2ce340bc0a48\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-27d21df8-653f-4533-820b-2ce340bc0a48\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data_extracted.zip to data_extracted.zip\n",
            "Extracted data_extracted.zip to /content/unzipped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BENCHMARK_PATH = '/content/unzipped/data_extracted/benchmarks/contractnli.json'\n",
        "CORPUS_DIR = '/content/unzipped/data_extracted/corpus'\n",
        "TRAINING_DATA_PATH = '/content/unzipped/data_extracted/top_100000_data.csv'\n",
        "OUTPUT_DIR = '/content/output'\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQW-lvp6GZ0D",
        "outputId": "7d573e90-5c9a-47ad-ae07-7c1090b54376"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output directory: /content/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing corpus and test data\n",
        "print(\"Preparing corpus and test data...\")\n",
        "print(f\"Loading benchmark from: {BENCHMARK_PATH}\")\n",
        "print(f\"Loading corpus from: {CORPUS_DIR}\")\n",
        "\n",
        "try:\n",
        "    corpus_passages, query_to_gold = prepare_corpus_from_benchmark(\n",
        "        BENCHMARK_PATH,\n",
        "        CORPUS_DIR,\n",
        "        chunk_size=500\n",
        "    )\n",
        "    print(f\"Corpus size: {len(corpus_passages)} passages\")\n",
        "    print(f\"Number of test queries: {len(query_to_gold)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error preparing corpus: {e}\")\n",
        "    print(\"Please ensure the paths are correct and files are uploaded to Colab\")\n",
        "    corpus_passages = []\n",
        "    query_to_gold = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X9xA_2IuY91",
        "outputId": "02d4e15d-ef2f-48cc-be04-388297bb2db9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing corpus and test data...\n",
            "Loading benchmark from: /content/unzipped/data_extracted/benchmarks/contractnli.json\n",
            "Loading corpus from: /content/unzipped/data_extracted/corpus\n",
            "Corpus size: 563 passages\n",
            "Number of test queries: 977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading test benchmarks\n",
        "try:\n",
        "    tests = load_test_benchmark(BENCHMARK_PATH)\n",
        "    print(f\"Loaded {len(tests)} test cases\")\n",
        "    print(f\"Sample query: {tests[0]['query'][:100]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading test benchmark: {e}\")\n",
        "    tests = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMwiocFPuf3v",
        "outputId": "be5041be-5f68-41f1-93c4-6e79c3fe9a98"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 977 test cases\n",
            "Sample query: Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document indicate tha...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Baseline: TF-IDF"
      ],
      "metadata": {
        "id": "zsBW7mLWukDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Ensuring NLTK tokenizers are available\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9Xq64QKvQfN",
        "outputId": "ce8255a8-0ef3-4918-94e9-22c206b37a11"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Simple Baseline: TF-IDF Retrieval\")\n",
        "\n",
        "if len(corpus_passages) > 0 and len(tests) > 0:\n",
        "    # Initializing TF-IDF retriever\n",
        "    tfidf_retriever = TFIDFRetriever(max_features=5000)\n",
        "\n",
        "    # Indexing corpus (limiting to first 10000 passages)\n",
        "    max_passages = min(10000, len(corpus_passages))\n",
        "    print(f\"Indexing {max_passages} passages...\")\n",
        "    tfidf_retriever.index(corpus_passages[:max_passages])\n",
        "\n",
        "    # Generating predictions (processing first 100 for demo)\n",
        "    num_tests = min(100, len(tests))\n",
        "    print(f\"Processing {num_tests} test queries...\")\n",
        "    tfidf_predictions = []\n",
        "    for i, test in enumerate(tests[:num_tests]):\n",
        "        if i % 10 == 0:\n",
        "            print(f\"  Processed {i}/{num_tests} queries...\")\n",
        "        query = test['query']\n",
        "        results = tfidf_retriever.retrieve(query, k=10)\n",
        "        retrieved_passages = [passage for passage, score in results]\n",
        "        tfidf_predictions.append({\n",
        "            'query': query,\n",
        "            'retrieved_passages': retrieved_passages\n",
        "        })\n",
        "\n",
        "    # Evaluating\n",
        "    tfidf_results = evaluate_retrieval(tfidf_predictions, tests[:num_tests], k=10)\n",
        "    print(\"TF-IDF Baseline Results:\")\n",
        "    for metric, value in tfidf_results.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    # Saving predictions\n",
        "    with open(f'{OUTPUT_DIR}/tfidf_predictions.json', 'w') as f:\n",
        "        json.dump(tfidf_predictions, f, indent=2)\n",
        "    print(f\"Predictions saved to {OUTPUT_DIR}/tfidf_predictions.json\")\n",
        "else:\n",
        "    print(\"Skipping TF-IDF baseline: corpus or tests not loaded\")\n",
        "    tfidf_results = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeBX85eLumFc",
        "outputId": "fbe15396-1245-429f-e354-7dbe6f204092"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple Baseline: TF-IDF Retrieval\n",
            "Indexing 563 passages...\n",
            "Processing 100 test queries...\n",
            "  Processed 0/100 queries...\n",
            "  Processed 10/100 queries...\n",
            "  Processed 20/100 queries...\n",
            "  Processed 30/100 queries...\n",
            "  Processed 40/100 queries...\n",
            "  Processed 50/100 queries...\n",
            "  Processed 60/100 queries...\n",
            "  Processed 70/100 queries...\n",
            "  Processed 80/100 queries...\n",
            "  Processed 90/100 queries...\n",
            "TF-IDF Baseline Results:\n",
            "  exact_match: 0.0000\n",
            "  span_f1: 0.2009\n",
            "  recall@10: 0.3717\n",
            "  ndcg@10: 0.2757\n",
            "  num_examples: 100.0000\n",
            "Predictions saved to /content/output/tfidf_predictions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Strong Baseline: BM25"
      ],
      "metadata": {
        "id": "Yo1OUdgewKlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Strong Baseline: BM25 Retrieval\")\n",
        "\n",
        "if len(corpus_passages) > 0 and len(tests) > 0:\n",
        "    # Initializing BM25 retriever\n",
        "    bm25_retriever = BM25Retriever(k1=1.5, b=0.75)\n",
        "\n",
        "    # Indexing corpus\n",
        "    max_passages = min(10000, len(corpus_passages))\n",
        "    print(f\"Indexing {max_passages} passages...\")\n",
        "    bm25_retriever.index(corpus_passages[:max_passages])\n",
        "\n",
        "    # Generating predictions\n",
        "    num_tests = min(100, len(tests))\n",
        "    print(f\"Processing {num_tests} test queries...\")\n",
        "    bm25_predictions = []\n",
        "    for i, test in enumerate(tests[:num_tests]):\n",
        "        if i % 10 == 0:\n",
        "            print(f\"  Processed {i}/{num_tests} queries...\")\n",
        "        query = test['query']\n",
        "        results = bm25_retriever.retrieve(query, k=10)\n",
        "        retrieved_passages = [passage for passage, score in results]\n",
        "        bm25_predictions.append({\n",
        "            'query': query,\n",
        "            'retrieved_passages': retrieved_passages\n",
        "        })\n",
        "\n",
        "    # Evaluating\n",
        "    bm25_results = evaluate_retrieval(bm25_predictions, tests[:num_tests], k=10)\n",
        "    print(\"BM25 Baseline Results:\")\n",
        "    for metric, value in bm25_results.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    # Saving predictions\n",
        "    with open(f'{OUTPUT_DIR}/bm25_predictions.json', 'w') as f:\n",
        "        json.dump(bm25_predictions, f, indent=2)\n",
        "    print(f\"Predictions saved to {OUTPUT_DIR}/bm25_predictions.json\")\n",
        "else:\n",
        "    print(\"Skipping BM25 baseline: corpus or tests not loaded\")\n",
        "    bm25_results = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_5WbMaqwIOW",
        "outputId": "d1271767-7a0c-4e38-c887-323da4166440"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Strong Baseline: BM25 Retrieval\n",
            "Indexing 563 passages...\n",
            "Processing 100 test queries...\n",
            "  Processed 0/100 queries...\n",
            "  Processed 10/100 queries...\n",
            "  Processed 20/100 queries...\n",
            "  Processed 30/100 queries...\n",
            "  Processed 40/100 queries...\n",
            "  Processed 50/100 queries...\n",
            "  Processed 60/100 queries...\n",
            "  Processed 70/100 queries...\n",
            "  Processed 80/100 queries...\n",
            "  Processed 90/100 queries...\n",
            "BM25 Baseline Results:\n",
            "  exact_match: 0.0000\n",
            "  span_f1: 0.2217\n",
            "  recall@10: 0.5267\n",
            "  ndcg@10: 0.4544\n",
            "  num_examples: 100.0000\n",
            "Predictions saved to /content/output/bm25_predictions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of Results\n"
      ],
      "metadata": {
        "id": "YWJT8ufFwSiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if tfidf_results and bm25_results:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"SUMMARY OF BASELINE PERFORMANCE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"{'Metric':<15}{'TF-IDF':>10}{'BM25':>10}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Exact Match':<15}{tfidf_results.get('exact_match', 0):>10.4f}{bm25_results.get('exact_match', 0):>10.4f}\")\n",
        "    print(f\"{'Span F1':<15}{tfidf_results.get('span_f1', 0):>10.4f}{bm25_results.get('span_f1', 0):>10.4f}\")\n",
        "    print(f\"{'Recall@10':<15}{tfidf_results.get('recall@10', 0):>10.4f}{bm25_results.get('recall@10', 0):>10.4f}\")\n",
        "    print(f\"{'nDCG@10':<15}{tfidf_results.get('ndcg@10', 0):>10.4f}{bm25_results.get('ndcg@10', 0):>10.4f}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    summary = {\n",
        "        'tfidf': tfidf_results,\n",
        "        'bm25': bm25_results\n",
        "    }\n",
        "    with open(f'{OUTPUT_DIR}/results_summary.json', 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print(f\"Results summary saved to {OUTPUT_DIR}/results_summary.json\")\n",
        "else:\n",
        "    print(\"Results not available. Please run the baseline cells above first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HohuurqwUw_",
        "outputId": "6b2b2b94-6c77-40c1-cc4e-3f438130eded"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SUMMARY OF BASELINE PERFORMANCE\n",
            "============================================================\n",
            "Metric             TF-IDF      BM25\n",
            "------------------------------------------------------------\n",
            "Exact Match        0.0000    0.0000\n",
            "Span F1            0.2009    0.2217\n",
            "Recall@10          0.3717    0.5267\n",
            "nDCG@10            0.2757    0.4544\n",
            "============================================================\n",
            "Results summary saved to /content/output/results_summary.json\n"
          ]
        }
      ]
    }
  ]
}