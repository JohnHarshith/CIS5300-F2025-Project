{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Milestone 3: Hybrid BM25 + Sentence-BERT Retrieval\n",
    "\n",
    "This notebook implements the hybrid retrieval extension that combines:\n",
    "1. **BM25 (lexical retrieval)** - Captures exact term matches and rare legal keywords\n",
    "2. **Sentence-BERT (semantic retrieval)** - Captures paraphrases and semantic similarity\n",
    "3. **Fusion strategies** - Weighted combination/Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "The hybrid approach improves over individual baselines by leveraging complementary signals."
   ],
   "metadata": {
    "id": "1aeZlJxTI_6Z"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup and Installation"
   ],
   "metadata": {
    "id": "LrrFVAsiJDCx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Installing required packages\n",
    "!pip install rank-bm25 nltk scikit-learn numpy pandas sentence-transformers torch -q"
   ],
   "metadata": {
    "id": "3MltkE0IJAIH"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Downloading stopwords\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except:\n",
    "    import ssl\n",
    "    try:\n",
    "        _create_unverified_https_context = ssl._create_unverified_context\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    else:\n",
    "        ssl._create_default_https_context = _create_unverified_https_context\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stop_words = set(stopwords.words('english'))"
   ],
   "metadata": {
    "id": "V1RpDC32JH0r"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utilities Functions"
   ],
   "metadata": {
    "id": "ac785cDXJIZR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_text(text: str, lower: bool = True) -> str:\n",
    "    \"\"\"Preprocess text for retrieval.\"\"\"\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize(text: str, remove_stopwords: bool = False) -> List[str]:\n",
    "    \"\"\"Tokenize text.\"\"\"\n",
    "    text = preprocess_text(text, lower=True)\n",
    "    tokens = word_tokenize(text)\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in stop_words and t.isalnum()]\n",
    "    else:\n",
    "        tokens = [t for t in tokens if t.isalnum()]\n",
    "    return tokens\n",
    "\n",
    "def load_test_benchmark(benchmark_path: str) -> List[dict]:\n",
    "    \"\"\"Load test benchmark JSON file.\"\"\"\n",
    "    with open(benchmark_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data.get('tests', data)\n",
    "\n",
    "def load_corpus_file(corpus_path: str) -> str:\n",
    "    \"\"\"Load a corpus text file.\"\"\"\n",
    "    with open(corpus_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        return f.read()\n",
    "\n",
    "def get_passage_from_span(text: str, span: List[int]) -> str:\n",
    "    \"\"\"Extract passage from text using character span.\"\"\"\n",
    "    start, end = span\n",
    "    return text[start:end]\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 500) -> List[str]:\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    step = max(1, chunk_size // 2)\n",
    "    for i in range(0, len(words), step):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "        if i + chunk_size >= len(words):\n",
    "            break\n",
    "    return chunks if chunks else [text]\n",
    "\n",
    "def prepare_corpus_from_benchmark(\n",
    "    benchmark_path: str,\n",
    "    corpus_dir: str,\n",
    "    chunk_size: int = 500) -> Tuple[List[str], Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Prepare corpus by chunking documents and mapping queries to gold passages.\n",
    "\n",
    "    Returns:\n",
    "        corpus_passages: List of all passages in corpus\n",
    "        query_to_gold: Mapping from query to list of gold answer passages\n",
    "    \"\"\"\n",
    "    tests = load_test_benchmark(benchmark_path)\n",
    "    corpus_passages = []\n",
    "    query_to_gold = {}\n",
    "    processed_files = set()\n",
    "\n",
    "    for test in tests:\n",
    "        query = test['query']\n",
    "        gold_answers = []\n",
    "\n",
    "        for snippet in test.get('snippets', []):\n",
    "            file_path = snippet['file_path']\n",
    "            span = snippet['span']\n",
    "\n",
    "            full_path = os.path.join(corpus_dir, file_path)\n",
    "            if os.path.exists(full_path):\n",
    "                doc_text = load_corpus_file(full_path)\n",
    "                gold_passage = get_passage_from_span(doc_text, span)\n",
    "                gold_answers.append(gold_passage)\n",
    "\n",
    "                if full_path not in processed_files:\n",
    "                    chunks = chunk_text(doc_text, chunk_size)\n",
    "                    corpus_passages.extend(chunks)\n",
    "                    processed_files.add(full_path)\n",
    "\n",
    "        if gold_answers:\n",
    "            query_to_gold[query] = gold_answers\n",
    "\n",
    "    return corpus_passages, query_to_gold"
   ],
   "metadata": {
    "id": "qitSs3UqJK3M"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hybrid Retriever Implementation"
   ],
   "metadata": {
    "id": "p3-2AcXNJO-w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SentenceBERTEncoder:\n",
    "    \"\"\"Encodes passages/queries with Sentence-BERT.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        batch_size: int = 32,\n",
    "        device: Optional[str] = None,\n",
    "        normalize_embeddings: bool = True,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.normalize_embeddings = normalize_embeddings\n",
    "        self.device = device\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        self.corpus_texts: List[str] = []\n",
    "        self.corpus_embeddings: Optional[np.ndarray] = None\n",
    "\n",
    "    def index(self, passages: List[str]):\n",
    "        \"\"\"Encode and store corpus passages.\"\"\"\n",
    "        if not passages:\n",
    "            raise ValueError(\"Cannot index empty passage list.\")\n",
    "        self.corpus_texts = passages\n",
    "        embeddings = self.model.encode(\n",
    "            passages,\n",
    "            batch_size=self.batch_size,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True,\n",
    "            normalize_embeddings=self.normalize_embeddings,\n",
    "        )\n",
    "        if not self.normalize_embeddings:\n",
    "            norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "            norms[norms == 0] = 1.0\n",
    "            embeddings = embeddings / norms\n",
    "        self.corpus_embeddings = embeddings.astype(np.float32)\n",
    "\n",
    "    def score(self, query: str) -> np.ndarray:\n",
    "        \"\"\"Score query against all corpus passages.\"\"\"\n",
    "        if self.corpus_embeddings is None:\n",
    "            raise ValueError(\"Index before scoring.\")\n",
    "        query_emb = self.model.encode(\n",
    "            [query],\n",
    "            batch_size=1,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=False,\n",
    "            normalize_embeddings=self.normalize_embeddings,\n",
    "        )[0]\n",
    "        if not self.normalize_embeddings:\n",
    "            denom = np.linalg.norm(query_emb) + 1e-12\n",
    "            query_emb = query_emb / denom\n",
    "        scores = np.dot(self.corpus_embeddings, query_emb)\n",
    "        return scores"
   ],
   "metadata": {
    "id": "TP9iHLrbJRmT"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class HybridRetriever:\n",
    "    \"\"\"\n",
    "    Combines BM25 and Sentence-BERT scores via fusion.\n",
    "    Supports two fusion methods: weighted combination and Reciprocal Rank Fusion (RRF).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bm25_k1: float = 1.5,\n",
    "        bm25_b: float = 0.75,\n",
    "        model_name: str = \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        dense_batch_size: int = 32,\n",
    "        fusion_method: str = \"weighted\",\n",
    "        bm25_weight: float = 0.55,\n",
    "        dense_weight: float = 0.45,\n",
    "        rrf_k: int = 60,\n",
    "        fusion_depth: int = 100,\n",
    "        device: Optional[str] = None,\n",
    "    ):\n",
    "        self.bm25_k1 = bm25_k1\n",
    "        self.bm25_b = bm25_b\n",
    "        self.fusion_method = fusion_method\n",
    "        self.bm25_weight = bm25_weight\n",
    "        self.dense_weight = dense_weight\n",
    "        self.rrf_k = rrf_k\n",
    "        self.fusion_depth = fusion_depth\n",
    "\n",
    "        self.bm25 = None\n",
    "        self.encoder = SentenceBERTEncoder(\n",
    "            model_name=model_name,\n",
    "            batch_size=dense_batch_size,\n",
    "            device=device,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        self.corpus_texts: List[str] = []\n",
    "        self.tokenized_passages: List[List[str]] = []\n",
    "\n",
    "    def index(self, passages: List[str]):\n",
    "        \"\"\"Index passages with both BM25 and Sentence-BERT.\"\"\"\n",
    "        self.corpus_texts = passages\n",
    "        self.tokenized_passages = [\n",
    "            tokenize(p, remove_stopwords=True) for p in passages\n",
    "        ]\n",
    "        self.bm25 = BM25Okapi(\n",
    "            self.tokenized_passages, k1=self.bm25_k1, b=self.bm25_b\n",
    "        )\n",
    "        self.encoder.index(passages)\n",
    "\n",
    "    def _normalize_scores(self, scores: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalize scores to [0, 1] range.\"\"\"\n",
    "        if scores.size == 0:\n",
    "            return scores\n",
    "        min_val = scores.min()\n",
    "        max_val = scores.max()\n",
    "        if max_val - min_val < 1e-9:\n",
    "            return np.ones_like(scores)\n",
    "        return (scores - min_val) / (max_val - min_val)\n",
    "\n",
    "    def _fuse_rrf(\n",
    "        self, bm25_scores: np.ndarray, dense_scores: np.ndarray\n",
    "    ) -> Dict[int, float]:\n",
    "        \"\"\"Reciprocal Rank Fusion (RRF).\"\"\"\n",
    "        rrf_scores: Dict[int, float] = defaultdict(float)\n",
    "\n",
    "        bm25_rank = np.argsort(bm25_scores)[::-1][:self.fusion_depth]\n",
    "        dense_rank = np.argsort(dense_scores)[::-1][:self.fusion_depth]\n",
    "\n",
    "        for rank, idx in enumerate(bm25_rank):\n",
    "            rrf_scores[int(idx)] += 1.0 / (self.rrf_k + rank + 1)\n",
    "\n",
    "        for rank, idx in enumerate(dense_rank):\n",
    "            rrf_scores[int(idx)] += 1.0 / (self.rrf_k + rank + 1)\n",
    "\n",
    "        return rrf_scores\n",
    "\n",
    "    def _fuse_weighted(\n",
    "        self, bm25_scores: np.ndarray, dense_scores: np.ndarray\n",
    "    ) -> Dict[int, float]:\n",
    "        \"\"\"Weighted combination of normalized scores.\"\"\"\n",
    "        bm25_norm = self._normalize_scores(bm25_scores)\n",
    "        dense_norm = self._normalize_scores(dense_scores)\n",
    "        combined = (\n",
    "            self.bm25_weight * bm25_norm + self.dense_weight * dense_norm\n",
    "        )\n",
    "        return {int(idx): float(score) for idx, score in enumerate(combined)}\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 10) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Retrieve top-k passages using hybrid fusion.\"\"\"\n",
    "        if self.bm25 is None or self.encoder.corpus_embeddings is None:\n",
    "            raise ValueError(\"Retriever must be indexed before retrieval.\")\n",
    "\n",
    "        bm25_scores = self.bm25.get_scores(\n",
    "            tokenize(query, remove_stopwords=True)\n",
    "        )\n",
    "        dense_scores = self.encoder.score(query)\n",
    "\n",
    "        if self.fusion_method == \"rrf\":\n",
    "            fused_scores = self._fuse_rrf(bm25_scores, dense_scores)\n",
    "        else:\n",
    "            fused_scores = self._fuse_weighted(bm25_scores, dense_scores)\n",
    "\n",
    "        ranked = sorted(\n",
    "            fused_scores.items(), key=lambda x: x[1], reverse=True\n",
    "        )[:k]\n",
    "        return [(self.corpus_texts[idx], score) for idx, score in ranked]"
   ],
   "metadata": {
    "id": "Oilytwr7JV1K"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation"
   ],
   "metadata": {
    "id": "4_HG3h-rJTwX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Uploading the ZIP file\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Extracting the uploaded ZIP file\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        with zipfile.ZipFile(io.BytesIO(uploaded[filename]), 'r') as zip_ref:\n",
    "            zip_ref.extractall('unzipped')\n",
    "        print(f\"Extracted {filename} to /content/unzipped\")\n",
    "    else:\n",
    "        print(f\"{filename} is not a ZIP file.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "G1gW-aT2JaBU",
    "outputId": "bb3fc0c3-73d8-4ef0-d614-ba153be98b6f"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-2c870c07-c420-46ec-bba0-6df74a6f879d\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-2c870c07-c420-46ec-bba0-6df74a6f879d\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving data_extracted.zip to data_extracted.zip\n",
      "Extracted data_extracted.zip to /content/unzipped\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "BENCHMARK_PATH = '/content/unzipped/data_extracted/benchmarks/contractnli.json'\n",
    "CORPUS_DIR = '/content/unzipped/data_extracted/corpus'\n",
    "OUTPUT_DIR = '/content/output'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Loading benchmark\n",
    "print(f\"Loading benchmark from {BENCHMARK_PATH}...\")\n",
    "tests = load_test_benchmark(BENCHMARK_PATH)\n",
    "print(f\"Loaded {len(tests)} test cases\")\n",
    "\n",
    "# Preparing corpus\n",
    "print(f\"Preparing corpus from {CORPUS_DIR}...\")\n",
    "corpus_passages, query_to_gold = prepare_corpus_from_benchmark(\n",
    "    BENCHMARK_PATH,\n",
    "    CORPUS_DIR,\n",
    "    chunk_size=500\n",
    ")\n",
    "print(f\"Corpus size: {len(corpus_passages)} passages\")\n",
    "print(f\"Number of test queries: {len(query_to_gold)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LqS7RzfKJii5",
    "outputId": "a9ce33c3-0f2e-4255-8604-262a7d283f45"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output directory: /content/output\n",
      "Loading benchmark from /content/unzipped/data_extracted/benchmarks/contractnli.json...\n",
      "Loaded 977 test cases\n",
      "Preparing corpus from /content/unzipped/data_extracted/corpus...\n",
      "Corpus size: 563 passages\n",
      "Number of test queries: 977\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running Hybrid Baseline"
   ],
   "metadata": {
    "id": "N8bmDmL8JsUM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "\n",
    "# Ensuring NLTK tokenizers are available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGZNad9hND8N",
    "outputId": "5cc13763-9389-4fe8-cfd5-c899b8bb7ef4"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Hybrid Baseline: BM25 + Sentence-BERT Fusion\")\n",
    "\n",
    "if len(corpus_passages) > 0 and len(tests) > 0:\n",
    "    # Initializing hybrid retriever with weighted fusion\n",
    "    # Tuned weights: 0.55 BM25, 0.45 Sentence-BERT\n",
    "    hybrid_retriever = HybridRetriever(\n",
    "        fusion_method=\"weighted\",\n",
    "        bm25_weight=0.55,\n",
    "        dense_weight=0.45,\n",
    "        fusion_depth=100,\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        dense_batch_size=32,\n",
    "        device=None,  # Auto-detect GPU if available\n",
    "    )\n",
    "\n",
    "    # Indexing corpus\n",
    "    print(f\"Indexing {len(corpus_passages)} passages with BM25 and Sentence-BERT...\")\n",
    "    hybrid_retriever.index(corpus_passages)\n",
    "\n",
    "    # Generating predictions\n",
    "    num_tests = len(tests) # Processing all queries\n",
    "    print(f\"Processing {num_tests} test queries...\")\n",
    "    hybrid_predictions = []\n",
    "    for i, test in enumerate(tests[:num_tests]):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Processed {i + 1}/{num_tests} queries...\")\n",
    "        query = test['query']\n",
    "        results = hybrid_retriever.retrieve(query, k=10)\n",
    "        retrieved_passages = [passage for passage, score in results]\n",
    "        hybrid_predictions.append({\n",
    "            'query': query,\n",
    "            'retrieved_passages': retrieved_passages\n",
    "        })\n",
    "\n",
    "    # Saving predictions\n",
    "    output_path = f'{OUTPUT_DIR}/hybrid_predictions.json'\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(hybrid_predictions, f, indent=2)\n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "else:\n",
    "    print(\"Skipping hybrid baseline: corpus or tests not loaded\")\n",
    "    hybrid_predictions = []"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7e0abd186b4142bb869b98e7ee61f447",
      "88bab0e009c64319b80986c823ea345c",
      "d2888e90c1eb46109cb60db28e3bfb98",
      "c2f6a04c09834fc2b09455ba6c68adbc",
      "1afb405f20d94680825db901c84f5acf",
      "cde6459911ea4bb8a4d67ae71bad42eb",
      "3d2811e223b445b7ba1f1b89ae32e6e3",
      "4590700ae83148b8b42a19b1d9c118e6",
      "0fac8806ab724b17a8505c1a5adda7c4",
      "b11d4797fe704b7e9bb8295fbd8bd2fd",
      "7a9d4347e35c445bb919f47552910519",
      "26f268839788429b86ea2ff2a39da5cb",
      "3ee2548a86374f5fb64eaf0d3a5cfd2c",
      "296b9ac66581493baa2ab53704198cf5",
      "655676e9df28490ba9dbcc70a0a6f579",
      "aacba4e8ba9e42b18abed3b5a925604e",
      "5e9f348f61754762bc8b38ff4d95a1d9",
      "a8fabd6e117843f6b555776a74a24279",
      "9d76c0703ead43dea8d1d97734f79adf",
      "e8653a77444b403bb3a68fee0696457b",
      "ed73bdab340846dda81bc500281927c9",
      "1e365abb2f5f47e1a5056b074dee7054",
      "3a539906ca6945b4987b98303817547d",
      "5ac5cba491534325880972ccb69de774",
      "55aee9c4eb56458fb794548ffac2b035",
      "35b8fe0c9f7d48fc86ecdb1797987dfd",
      "487e0c3947dc48988436c62ed2e780bc",
      "105bf0ed29a64d0b8815fec6f1a4e102",
      "8618955ad17c471cbce8ce54ab2acc05",
      "b0b276efc0a843d2aa9f61fabdf7c0da",
      "cbeb79fa6542498283fcc90af67663ee",
      "ff3b863a08c247da85d10a42f33e2164",
      "d9bfe7a1477e480eb78e37c166009c3e",
      "f378389ccfae433aaedca2817f9d1d10",
      "d9510456993849c7a4fb9e4a89c83082",
      "48d9221346f94225a26e1442cc84632e",
      "0b229b754e454276b8d7ee3ff6d581b4",
      "a3e2e6f456bd489d8be56ba71d3db47d",
      "66000fb9547b400f84d55fb25cce7865",
      "ef5e8b2931624d5b9cabd7e43b14baed",
      "d407ea5f50004c9ca897c95c84d8f7b0",
      "aa61791507df4925bc5f9b5a689c294a",
      "a1fe9a995ce2403e9765a70f5f9a90d4",
      "697ad205f4d849edad264dc08efe2d22",
      "b4793b97c7044b0d91871ae1c595fb29",
      "7e9f7a630dba44e1a379ff9fcbefc189",
      "b83a219754f443bf9d0eca4da290d3f8",
      "2d064628d1364d5189c597444174a3e1",
      "32a7267a89114cc484c7ff3c0d038268",
      "ae775979dbe74ae392f234c4ac3e1fec",
      "2544b69ec64e445eba1ebcbc17543fd7",
      "13b0751db72f4d508ca24e8cf96a4cba",
      "5e8c6f2e16974553868fa19a0271d15a",
      "15846a71264f4441a5fc8d8722ba1f73",
      "184eac1767484a0395f2646406b404a7",
      "d5d79bd7abe74eaf9456c50d2e441d64",
      "ea8c626e8c124f7492a6f68b26d21bf1",
      "e1974d46e51d4d9e9210e5036076d366",
      "79160d9f3a4149c5ac007f0f797e71db",
      "b3c7168334374117b59244395b948d99",
      "07c8556f65804b1c8bab09f046fffbc6",
      "39bb14c600f3421d887cd5fca082ca10",
      "8d0f427681804406801b2be2026b99b4",
      "8dbd11276de84ffc82a5a38c9c09ca6d",
      "babc5d8049e942ccbeec49f6dacec649",
      "d34374a48a0448839d2576efb7da80a7",
      "9f63bab6836c4457b3f685ac403b5be3",
      "95442ea102524e3bb266acfe14198d45",
      "bcc45a56b0b940a7afea0d3b1ec5527b",
      "d55c5da455d14ab48ce35fd82faf98d5",
      "25de81cd2c6441a683d4e4442e23e7a4",
      "1c0512c8578b4f13acdf05936d6d1278",
      "da750fe8829247d8b9cf4e7973f0d286",
      "7e6e1ee9cd954206809cb1c7832ab163",
      "694c1ead241b456cbd728fa1e04a181d",
      "3bf9ec95d52040f394eb328547220c97",
      "78971e7bf99e4cd3b22f4cf02ccd8db0",
      "3b78ab2f90054f5892fa7d0070a7c043",
      "87e74893d1b245568c47fd39b019191e",
      "9f354f31df9e42dc9f5da32ab13b068b",
      "65ae4899597347c4b1bcb256f5a4a4a1",
      "dcb146575a784fb0853faf6beadcaa61",
      "4549251c5a9f49b3b74a34b7bf056a73",
      "0210422188a643c4bb39bfaf3540e690",
      "3d68edefd00b4f929276a0644c68d7ce",
      "600bbec798504531aaf612c2fb3f1120",
      "0deac91e962e4e68b189a9440df34ff1",
      "8dac3228b9d346ccae060f753df3ac16",
      "0c9f5789b84d49258ba3b20af93b1eaa",
      "d3a314570cc549ddb2e1b7c1138c9e44",
      "8eea7a8fc018412d85c498e15093c138",
      "0589413335ed45b3be43cf20b634920c",
      "51914909a8de4286b00ef6c357e3921d",
      "d9f07edc77b541e0a2f118d3cae7cb91",
      "709abe9ba2354f559ed87cf665afd77b",
      "a2aaae6d0e43424b9479cc05a9940769",
      "ad1d5f4541fd45788d59f39d3dd9aabc",
      "f9572e1299c84309923ca7b3916da872",
      "6cf3545aff0b469f9a432c55e92df758",
      "b6b046fb99c24b5f81343be60330f059",
      "d9f3cc9c25d149348256e3abc8de6ffd",
      "b8701f3e9f9643a7b2fa6e597f18bdc2",
      "cb2c6c63d6ba4107a1e913068a5322a2",
      "c3724b3e9c2e4e1fbaef27670e703264",
      "4574cf9d1b4f41ee8d42b43a819b29f2",
      "b3a19805c8d540179325de9d69498f63",
      "278b09a32e874baaaf36c57a9863e9f0",
      "d7c89a288c994df8a88f4640e49a81e4",
      "0030c8730bf34c7f8ab64872ded7f06d",
      "9b1e48f2992e4de6a45c56c18fce40e3",
      "89ae4c3300894b7d86e2eaabce404f12",
      "2fa98bd889ae4c8489d1d003ec71050a",
      "75ba071641a54a7c82c20bb4c5066ee6",
      "794c1abaa2e242bfb24265270eaa8536",
      "5770b87b66e04e8b8823864dc2e1a24b",
      "c8e87e8520254ce5864445e02133af09",
      "2dd121285adf4d02a0dcc085df396ef6",
      "dd9bac0c1d9a460b82c22561eee4e3df",
      "839ccc1466fe4940b6a664b10cf616cb",
      "8e3ed7347bf941268d59e5c4bdc3466d",
      "338142d5af654be4b2ab96a4680ae15e",
      "d81cf5c546664f7a95a8328444532494",
      "75049b27663b4481b6bc01aa3a03f4c6",
      "102e148c0e7f43478d86d4a173e3fe1d",
      "bb215e1cc71c4e7cb229e51297245384",
      "cd30e15d604940e296b530f7ebb1691b",
      "fcfd3492b54b44ea9fdf00a774277ede",
      "4934c91dbde540ec984b058fa3afaaa6",
      "e76e82d576d44add8650bd2d2f98220a",
      "e1eba67cfb034960a8cdc8c139879d8a",
      "2fd96e95f8c9421f9254986770a5fedb",
      "0ce91537cba2402a908406d643603ad0"
     ]
    },
    "id": "QmxmkBCXJuVv",
    "outputId": "c189e5f0-9443-45a1-b49c-6495a309facf"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hybrid Baseline: BM25 + Sentence-BERT Fusion\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e0abd186b4142bb869b98e7ee61f447"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26f268839788429b86ea2ff2a39da5cb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a539906ca6945b4987b98303817547d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f378389ccfae433aaedca2817f9d1d10"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4793b97c7044b0d91871ae1c595fb29"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5d79bd7abe74eaf9456c50d2e441d64"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f63bab6836c4457b3f685ac403b5be3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b78ab2f90054f5892fa7d0070a7c043"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c9f5789b84d49258ba3b20af93b1eaa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6b046fb99c24b5f81343be60330f059"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89ae4c3300894b7d86e2eaabce404f12"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Indexing 563 passages with BM25 and Sentence-BERT...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d81cf5c546664f7a95a8328444532494"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing 977 test queries...\n",
      "  Processed 10/977 queries...\n",
      "  Processed 20/977 queries...\n",
      "  Processed 30/977 queries...\n",
      "  Processed 40/977 queries...\n",
      "  Processed 50/977 queries...\n",
      "  Processed 60/977 queries...\n",
      "  Processed 70/977 queries...\n",
      "  Processed 80/977 queries...\n",
      "  Processed 90/977 queries...\n",
      "  Processed 100/977 queries...\n",
      "  Processed 110/977 queries...\n",
      "  Processed 120/977 queries...\n",
      "  Processed 130/977 queries...\n",
      "  Processed 140/977 queries...\n",
      "  Processed 150/977 queries...\n",
      "  Processed 160/977 queries...\n",
      "  Processed 170/977 queries...\n",
      "  Processed 180/977 queries...\n",
      "  Processed 190/977 queries...\n",
      "  Processed 200/977 queries...\n",
      "  Processed 210/977 queries...\n",
      "  Processed 220/977 queries...\n",
      "  Processed 230/977 queries...\n",
      "  Processed 240/977 queries...\n",
      "  Processed 250/977 queries...\n",
      "  Processed 260/977 queries...\n",
      "  Processed 270/977 queries...\n",
      "  Processed 280/977 queries...\n",
      "  Processed 290/977 queries...\n",
      "  Processed 300/977 queries...\n",
      "  Processed 310/977 queries...\n",
      "  Processed 320/977 queries...\n",
      "  Processed 330/977 queries...\n",
      "  Processed 340/977 queries...\n",
      "  Processed 350/977 queries...\n",
      "  Processed 360/977 queries...\n",
      "  Processed 370/977 queries...\n",
      "  Processed 380/977 queries...\n",
      "  Processed 390/977 queries...\n",
      "  Processed 400/977 queries...\n",
      "  Processed 410/977 queries...\n",
      "  Processed 420/977 queries...\n",
      "  Processed 430/977 queries...\n",
      "  Processed 440/977 queries...\n",
      "  Processed 450/977 queries...\n",
      "  Processed 460/977 queries...\n",
      "  Processed 470/977 queries...\n",
      "  Processed 480/977 queries...\n",
      "  Processed 490/977 queries...\n",
      "  Processed 500/977 queries...\n",
      "  Processed 510/977 queries...\n",
      "  Processed 520/977 queries...\n",
      "  Processed 530/977 queries...\n",
      "  Processed 540/977 queries...\n",
      "  Processed 550/977 queries...\n",
      "  Processed 560/977 queries...\n",
      "  Processed 570/977 queries...\n",
      "  Processed 580/977 queries...\n",
      "  Processed 590/977 queries...\n",
      "  Processed 600/977 queries...\n",
      "  Processed 610/977 queries...\n",
      "  Processed 620/977 queries...\n",
      "  Processed 630/977 queries...\n",
      "  Processed 640/977 queries...\n",
      "  Processed 650/977 queries...\n",
      "  Processed 660/977 queries...\n",
      "  Processed 670/977 queries...\n",
      "  Processed 680/977 queries...\n",
      "  Processed 690/977 queries...\n",
      "  Processed 700/977 queries...\n",
      "  Processed 710/977 queries...\n",
      "  Processed 720/977 queries...\n",
      "  Processed 730/977 queries...\n",
      "  Processed 740/977 queries...\n",
      "  Processed 750/977 queries...\n",
      "  Processed 760/977 queries...\n",
      "  Processed 770/977 queries...\n",
      "  Processed 780/977 queries...\n",
      "  Processed 790/977 queries...\n",
      "  Processed 800/977 queries...\n",
      "  Processed 810/977 queries...\n",
      "  Processed 820/977 queries...\n",
      "  Processed 830/977 queries...\n",
      "  Processed 840/977 queries...\n",
      "  Processed 850/977 queries...\n",
      "  Processed 860/977 queries...\n",
      "  Processed 870/977 queries...\n",
      "  Processed 880/977 queries...\n",
      "  Processed 890/977 queries...\n",
      "  Processed 900/977 queries...\n",
      "  Processed 910/977 queries...\n",
      "  Processed 920/977 queries...\n",
      "  Processed 930/977 queries...\n",
      "  Processed 940/977 queries...\n",
      "  Processed 950/977 queries...\n",
      "  Processed 960/977 queries...\n",
      "  Processed 970/977 queries...\n",
      "Predictions saved to /content/output/hybrid_predictions.json\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "id": "tY5rDKuPJw1m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluation functions (from score.py)\n",
    "def exact_match(predicted: str, gold: str) -> bool:\n",
    "    \"\"\"Check if predicted text exactly matches gold text.\"\"\"\n",
    "    return predicted.strip().lower() == gold.strip().lower()\n",
    "\n",
    "def span_f1(predicted: str, gold: str) -> float:\n",
    "    \"\"\"Compute F1 score based on token overlap.\"\"\"\n",
    "    pred_tokens = set(tokenize(predicted, remove_stopwords=False))\n",
    "    gold_tokens = set(tokenize(gold, remove_stopwords=False))\n",
    "\n",
    "    if len(pred_tokens) == 0 and len(gold_tokens) == 0:\n",
    "        return 1.0\n",
    "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    intersection = pred_tokens & gold_tokens\n",
    "    precision = len(intersection) / len(pred_tokens) if len(pred_tokens) > 0 else 0.0\n",
    "    recall = len(intersection) / len(gold_tokens) if len(gold_tokens) > 0 else 0.0\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def recall_at_k(retrieved_passages: List[str], gold_passages: List[str], k: int = 10) -> float:\n",
    "    \"\"\"Compute Recall@K: fraction of gold passages found in top K.\"\"\"\n",
    "    if len(gold_passages) == 0:\n",
    "        return 1.0 if len(retrieved_passages) == 0 else 0.0\n",
    "\n",
    "    top_k = retrieved_passages[:k]\n",
    "    gold_normalized = [preprocess_text(g).strip() for g in gold_passages]\n",
    "    retrieved_normalized = [preprocess_text(r).strip() for r in top_k]\n",
    "\n",
    "    matches = 0\n",
    "    for gold in gold_normalized:\n",
    "        for ret in retrieved_normalized:\n",
    "            if gold in ret or ret in gold or gold == ret:\n",
    "                matches += 1\n",
    "                break\n",
    "\n",
    "    return matches / len(gold_passages)\n",
    "\n",
    "def ndcg_at_k(retrieved_passages: List[str], gold_passages: List[str], k: int = 10) -> float:\n",
    "    \"\"\"Compute Normalized Discounted Cumulative Gain (nDCG) at K.\"\"\"\n",
    "    if len(gold_passages) == 0:\n",
    "        return 1.0 if len(retrieved_passages) == 0 else 0.0\n",
    "\n",
    "    top_k = retrieved_passages[:k]\n",
    "    gold_normalized = [preprocess_text(g).strip() for g in gold_passages]\n",
    "    retrieved_normalized = [preprocess_text(r).strip() for r in top_k]\n",
    "\n",
    "    relevances = []\n",
    "    for ret in retrieved_normalized:\n",
    "        is_relevant = False\n",
    "        for gold in gold_normalized:\n",
    "            if gold in ret or ret in gold or gold == ret:\n",
    "                is_relevant = True\n",
    "                break\n",
    "        relevances.append(1.0 if is_relevant else 0.0)\n",
    "\n",
    "    dcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(relevances))\n",
    "    num_relevant = int(min(sum(relevances), k))\n",
    "    idcg = sum(1.0 / np.log2(i + 2) for i in range(num_relevant))\n",
    "\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return dcg / idcg\n",
    "\n",
    "def evaluate_retrieval(\n",
    "    predictions: List[Dict],\n",
    "    gold_standard: List[Dict],\n",
    "    k: int = 10\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate retrieval system performance.\"\"\"\n",
    "    if len(predictions) != len(gold_standard):\n",
    "        raise ValueError(f\"Mismatch: {len(predictions)} predictions vs {len(gold_standard)} gold examples\")\n",
    "\n",
    "    exact_matches = []\n",
    "    span_f1_scores = []\n",
    "    recall_at_k_scores = []\n",
    "    ndcg_at_k_scores = []\n",
    "\n",
    "    for pred, gold in zip(predictions, gold_standard):\n",
    "        gold_answers = [snippet['answer'] for snippet in gold.get('snippets', [])]\n",
    "\n",
    "        if len(gold_answers) == 0:\n",
    "            continue\n",
    "\n",
    "        retrieved = pred.get('retrieved_passages', [])\n",
    "\n",
    "        if len(retrieved) == 0:\n",
    "            exact_matches.append(0.0)\n",
    "            span_f1_scores.append(0.0)\n",
    "            recall_at_k_scores.append(0.0)\n",
    "            ndcg_at_k_scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        top_pred = retrieved[0] if retrieved else \"\"\n",
    "        em = any(exact_match(top_pred, gold_ans) for gold_ans in gold_answers)\n",
    "        exact_matches.append(1.0 if em else 0.0)\n",
    "\n",
    "        best_f1 = max([span_f1(top_pred, gold_ans) for gold_ans in gold_answers])\n",
    "        span_f1_scores.append(best_f1)\n",
    "\n",
    "        rec_k = recall_at_k(retrieved, gold_answers, k=k)\n",
    "        recall_at_k_scores.append(rec_k)\n",
    "\n",
    "        ndcg_k = ndcg_at_k(retrieved, gold_answers, k=k)\n",
    "        ndcg_at_k_scores.append(ndcg_k)\n",
    "\n",
    "    return {\n",
    "        'exact_match': np.mean(exact_matches),\n",
    "        'span_f1': np.mean(span_f1_scores),\n",
    "        f'recall@{k}': np.mean(recall_at_k_scores),\n",
    "        f'ndcg@{k}': np.mean(ndcg_at_k_scores),\n",
    "        'num_examples': len(predictions)\n",
    "    }"
   ],
   "metadata": {
    "id": "NQuuDwndJzPh"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluating hybrid baseline predictions\n",
    "if hybrid_predictions:\n",
    "    hybrid_results = evaluate_retrieval(hybrid_predictions, tests[:len(hybrid_predictions)], k=10)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"HYBRID BASELINE RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    for metric, value in hybrid_results.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Save results\n",
    "    results_path = f'{OUTPUT_DIR}/hybrid_results.json'\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(hybrid_results, f, indent=2)\n",
    "    print(f\"Results saved to {results_path}\")\n",
    "else:\n",
    "    print(\"No predictions to evaluate. Please run the hybrid baseline cell first.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gdNMytrJ5hi",
    "outputId": "729db8d5-0792-42dc-fa7b-2cecd88e690f"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================\n",
      "HYBRID BASELINE RESULTS\n",
      "============================================================\n",
      "  exact_match: 0.0000\n",
      "  span_f1: 0.2357\n",
      "  recall@10: 0.5511\n",
      "  ndcg@10: 0.4808\n",
      "  num_examples: 977.0000\n",
      "============================================================\n",
      "Results saved to /content/output/hybrid_results.json\n"
     ]
    }
   ]
  }
 ]
}